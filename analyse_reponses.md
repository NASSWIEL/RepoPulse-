# Analyse du Code - R√©ponses aux Questions

## 1. √âtiquetage des donn√©es

### R√©ponse :
Les donn√©es sont √©tiquet√©es de mani√®re **supervis√©e** pour un apprentissage sur s√©ries temporelles.

### Impl√©mentation :
- **Fichier** : `src/neural_network.py`, m√©thode `_create_sequences()` (lignes 230-295)
- **M√©canisme** : 
  - Pour chaque √©chantillon, on utilise une fen√™tre glissante de taille `lookback_window`
  - **Features (X)** : valeurs de commits des semaines `[t-lookback, ..., t-1]`
  - **Label (y)** : valeur de commits √† la semaine `t`
  
### Code pertinent :
```python
# src/neural_network.py, ligne 294-295
X = np.hstack(feature_list)
y = np.array([commits[i + lookback] for i in range(n_samples)])
```

### Conclusion :
Il s'agit d'un **apprentissage supervis√© sur s√©ries temporelles** sans labels externes. Le label est automatiquement g√©n√©r√© √† partir de la valeur suivante dans la s√©rie temporelle.

---

## 2. Normalisation des donn√©es

### R√©ponse :
La m√©thode de normalisation appliqu√©e est **Min-Max Scaling**, **PAS** une normalisation statistique.

### Formule exacte impl√©ment√©e :
```
Valeur normalis√©e = (Valeur originale - Minimum) / (Maximum - Minimum)
```

Cette formule ram√®ne les valeurs dans l'intervalle **[0, 1]**.

### Impl√©mentation :
- **Fichier** : `src/neural_network.py`, m√©thode `_normalize_data()` (lignes 298-315)
- **Code** :
```python
from sklearn.preprocessing import MinMaxScaler

def _normalize_data(self, data: np.ndarray, key: str, fit: bool = True) -> np.ndarray:
    """Normalize data using min-max scaling."""
    from sklearn.preprocessing import MinMaxScaler
    
    data = data.reshape(-1, 1) if len(data.shape) == 1 else data
    
    if fit:
        self.scalers[key] = MinMaxScaler()
        return self.scalers[key].fit_transform(data).flatten()
    else:
        if key not in self.scalers:
            return data.flatten()
        return self.scalers[key].transform(data).flatten()
```

### V√©rification :
**NON**, cela ne correspond **PAS** √† une normalisation statistique (moyenne nulle, variance unitaire).
- Normalisation statistique : `(X - Œº) / œÉ` ‚Üí intervalle non born√©
- Min-Max scaling (utilis√©e) : `(X - min) / (max - min)` ‚Üí intervalle [0, 1]

**Note** : Pour les features engineered, il y a une utilisation de `StandardScaler` (normalisation statistique) dans certains cas (ligne 414 de `neural_network.py`), mais pour les valeurs de commits brutes, c'est bien MinMaxScaler qui est utilis√©.

---

## 3. Fen√™tre temporelle (sliding window)

### R√©ponse :
La taille de la fen√™tre varie selon la configuration :
- **Par d√©faut dans `neural_network.py`** : `lookback_window = 24` semaines
- **Par d√©faut dans `train_neural_network.py`** : `lookback_window = 12` semaines  
- **Dans le notebook** : `LOOKBACK_WINDOW = 12` semaines

### Impl√©mentation :
- **Fichier** : `src/neural_network.py`, m√©thode `_create_sequences()` (lignes 230-295)

### Code :
```python
def _create_sequences(
    self, 
    commits: np.ndarray, 
    prs: Optional[np.ndarray] = None,
    lookback: int = 24
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Create input sequences for the neural network with feature engineering.
    """
    n_samples = len(commits) - lookback
    
    # Raw lookback features
    raw_features = []
    for i in range(n_samples):
        raw_features.append(commits[i:i + lookback])  # Fen√™tre glissante
    raw_features = np.array(raw_features)
    
    # Target values
    y = np.array([commits[i + lookback] for i in range(n_samples)])
    
    return X, y
```

### M√©canisme :
Pour cr√©er les s√©quences :
1. On parcourt la s√©rie temporelle avec une fen√™tre glissante de taille `lookback_window`
2. Pour chaque position `i` :
   - **Input** : `commits[i:i+lookback]` (ex: semaines 0 √† 11)
   - **Output** : `commits[i+lookback]` (ex: semaine 12)
3. La fen√™tre se d√©place d'un pas √† chaque it√©ration

---

## 4. M√©thodes de r√©f√©rence (baselines)

### R√©ponse :
**NON**, les deux m√©thodes de baseline ne sont **PAS impl√©ment√©es** dans le code.

### Recherche effectu√©e :
- Recherche de mots-cl√©s : `persistence`, `last value`, `baseline`, `moving average`, `naive`
- Fichiers examin√©s : tous les fichiers Python du projet
- R√©sultat : Aucune impl√©mentation explicite trouv√©e

### Conclusion :
- **M√©thode 1 (Persistance/Last Value)** : ‚ùå Non impl√©ment√©e
- **M√©thode 2 (Moyenne mobile/Moving Average)** : ‚ùå Non impl√©ment√©e

**Note** : Il y a bien une mention de "Last value" dans les features engineered (ligne 206 de `neural_network.py`), mais ce n'est pas une m√©thode baseline de pr√©diction, c'est simplement une feature ajout√©e au mod√®le.

---

## 5. Architecture du mod√®le

### R√©ponse IMPORTANTE :
Le mod√®le impl√©ment√© n'est **PAS un GRU**, c'est un **MLP (Multi-Layer Perceptron)** simple.

### Architecture compl√®te :

#### Version PyTorch (fichier `train_neural_network.py`, lignes 304-312) :
```python
# Build PyTorch model
layers = []
input_size = X_train.shape[1]  # D√©pend du lookback et des features
for hidden_size in hidden_layers:
    layers.append(nn.Linear(input_size, hidden_size))
    layers.append(nn.ReLU())
    layers.append(nn.Dropout(self.config.dropout_rate))
    input_size = hidden_size
layers.append(nn.Linear(input_size, 1))
model = nn.Sequential(*layers)
```

#### Structure typique :
```
Input Layer (size = lookback_window √ó nombre_de_s√©ries)
    ‚Üì
Linear(input_size ‚Üí hidden_layer[0])
    ‚Üì
ReLU()
    ‚Üì
Dropout(rate=0.2-0.4)
    ‚Üì
Linear(hidden_layer[0] ‚Üí hidden_layer[1])
    ‚Üì
ReLU()
    ‚Üì
Dropout(rate=0.2-0.4)
    ‚Üì
[... r√©p√©t√© pour chaque couche cach√©e ...]
    ‚Üì
Linear(hidden_layer[-1] ‚Üí 1)
    ‚Üì
Output (1 valeur : pr√©diction des commits)
```

### Dimensions des couches cach√©es :
Plusieurs configurations possibles selon les param√®tres :

#### Configuration 1 (par d√©faut auto-scale avec peu de donn√©es) :
- **Couches** : [64, 32]
- **Exemple complet** :
  - Input: 24 (si lookback=24, sans PRs)
  - Hidden 1: 64 neurones
  - Hidden 2: 32 neurones  
  - Output: 1 neurone

#### Configuration 2 (notebook) :
- **Couches** : [128, 64, 32]
- **Exemple complet** :
  - Input: 12 (si lookback=12, sans PRs)
  - Hidden 1: 128 neurones
  - Hidden 2: 64 neurones
  - Hidden 3: 32 neurones
  - Output: 1 neurone

### Nombre total de param√®tres entra√Ænables :
Pour calculer le nombre de param√®tres, on utilise la formule pour chaque couche Linear :
```
Param√®tres = (input_size √ó output_size) + output_size (bias)
```

#### Exemple concret (configuration [64, 32] avec lookback=24, sans PRs) :
1. **Linear(24 ‚Üí 64)** : (24 √ó 64) + 64 = **1,600 param√®tres**
2. **Dropout** : 0 param√®tres
3. **Linear(64 ‚Üí 32)** : (64 √ó 32) + 32 = **2,080 param√®tres**
4. **Dropout** : 0 param√®tres
5. **Linear(32 ‚Üí 1)** : (32 √ó 1) + 1 = **33 param√®tres**

**Total : 3,713 param√®tres entra√Ænables**

#### Exemple avec PRs (input = 24√ó2 = 48 features) :
1. **Linear(48 ‚Üí 64)** : (48 √ó 64) + 64 = **3,136 param√®tres**
2. **Linear(64 ‚Üí 32)** : (64 √ó 32) + 32 = **2,080 param√®tres**
3. **Linear(32 ‚Üí 1)** : (32 √ó 1) + 1 = **33 param√®tres**

**Total : 5,249 param√®tres entra√Ænables**

---

## 6. Impl√©mentation du mod√®le

### R√©ponse :
Le mod√®le n'est **PAS un GRU**. C'est un **MLP feedforward simple**.

### Caract√©ristiques :
- **Type** : MLP simple (Multi-Layer Perceptron)
- **Non empil√©** : Pas de stacked layers (contrairement aux stacked LSTM/GRU)
- **Non bidirectionnel** : Unidirectionnel (feedforward uniquement)

### M√©canismes suppl√©mentaires pr√©sents :

#### 1. Dropout (fichier `neural_network.py`, ligne 336)
```python
layers.append(nn.Dropout(self.config.dropout_rate))
```
- Taux par d√©faut : 0.4 (ligne 44 de `neural_network.py`)
- Active pendant l'entra√Ænement, d√©sactiv√©e pendant l'inf√©rence

#### 2. Batch Normalization (optionnel)
- **Configuration** : `use_batch_norm: bool = True` (ligne 50)
- Mais **non impl√©ment√©** dans le code PyTorch actuel
- Impl√©ment√© uniquement dans le notebook (lignes 89-90) :
```python
layers.append(nn.BatchNorm1d(hidden_size))
```

#### 3. Ensemble de mod√®les (ligne 521 de `neural_network.py`)
```python
# Train ensemble with different random seeds
for i in range(self.config.n_ensemble):
    torch.manual_seed(42 + i)
    np.random.seed(42 + i)
    model = self._train_torch_model(X, y, hidden_layers)
    self.models.append(model)
```
- **n_ensemble = 5** par d√©faut
- Utilis√© pour calculer des intervalles de confiance

#### 4. L2 Regularization
- **Configuration** : `l2_regularization: float = 0.01` (ligne 49)
- Appliqu√© dans sklearn MLPRegressor (ligne 428 de `neural_network.py`) :
```python
nn_model = MLPRegressor(
    ...
    alpha=self.config.l2_regularization,  # L2 regularization
    ...
)
```

#### 5. Noise Injection (ligne 417)
```python
if self.config.noise_injection > 0:
    noise = np.random.normal(0, self.config.noise_injection, X_scaled.shape)
    X_noisy = X_scaled + noise
```
- Taux par d√©faut : 0.05
- Appliqu√© pendant l'entra√Ænement pour robustesse

#### 6. Learning Rate Scheduling (ligne 317 de `train_neural_network.py`)
```python
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=5
)
```

#### 7. Early Stopping (ligne 322)
```python
best_val_loss = float('inf')
patience_counter = 0
# ...
if patience_counter >= self.config.early_stopping_patience:
    logger.info(f"Early stopping at epoch {epoch + 1}")
    break
```
- Patience par d√©faut : 30 epochs

### Version fallback (sklearn) :
Quand PyTorch n'est pas disponible, le code utilise un **ensemble de mod√®les** (ligne 439-450) :
- MLPRegressor (neural network)
- GradientBoostingRegressor
- Ridge regression

---

## 7. D√©tail de la normalisation

### Formule :
```
Valeur normalis√©e = (Valeur originale - Minimum) / (Maximum - Minimum)
```

**Note** : Ce n'est pas la formule demand√©e `(X - Œº) / œÉ` car le code utilise MinMaxScaler, pas StandardScaler.

### Sur quelles donn√©es sont calcul√©s min et max ?

#### R√©ponse : **Sur le jeu d'entra√Ænement uniquement**

### Impl√©mentation d√©taill√©e :

#### Fichier : `src/train_neural_network.py`, lignes 233-240
```python
# Normalize data
commit_scaler = MinMaxScaler()
train_commits_norm = commit_scaler.fit_transform(
    train_commits.reshape(-1, 1)
).flatten()

all_commits_norm = commit_scaler.transform(
    all_commits.reshape(-1, 1)
).flatten()
```

### Processus :
1. **Entra√Ænement** :
   - `fit_transform()` sur `train_commits` ‚Üí calcule min et max sur les donn√©es d'entra√Ænement
   - Les valeurs min et max sont stock√©es dans `commit_scaler`

2. **Validation/Test** :
   - `transform()` sur les donn√©es de validation ‚Üí utilise les min et max calcul√©s sur l'entra√Ænement
   - **PAS de nouveau calcul** de statistiques

3. **Inf√©rence** :
   - Les m√™mes min et max de l'entra√Ænement sont utilis√©s (ligne 550 de `neural_network.py`) :
```python
commits_norm = self._normalize_data(commits, "commits", fit=False)
```

### Pourquoi c'est important :
- √âvite la **fuite d'information (data leakage)** des donn√©es de validation vers l'entra√Ænement
- Garantit que le mod√®le voit des donn√©es dans la m√™me √©chelle en production
- Standard en machine learning pour les s√©ries temporelles

### Stockage des scalers :
Les scalers sont sauvegard√©s avec le mod√®le (ligne 432 de `train_neural_network.py`) :
```python
"scalers": {
    "commits": commit_scaler,
    "prs": prs_scaler
},
```

---

## R√©sum√© des Points Cl√©s

### ‚úÖ Ce qui est impl√©ment√© :
1. Apprentissage supervis√© sur s√©ries temporelles
2. Min-Max Scaling (normalisation [0,1])
3. Fen√™tre glissante de 12 ou 24 semaines
4. **MLP simple** (pas de GRU)
5. Dropout, L2 regularization, noise injection
6. Ensemble de mod√®les pour intervalles de confiance
7. Early stopping et learning rate scheduling

### ‚ùå Ce qui N'est PAS impl√©ment√© :
1. **GRU ou LSTM** (c'est un MLP)
2. **M√©thodes baseline** (Persistence, Moving Average)
3. Normalisation statistique (Œº=0, œÉ=1) pour les commits
4. Architecture bidirectionnelle
5. Stacked/empil√© layers (au sens RNN)

### üìä Configuration typique :
```
Input: 24 features (lookback=24 weeks)
   ‚Üì
Dense(24 ‚Üí 64) + ReLU + Dropout(0.4)
   ‚Üì
Dense(64 ‚Üí 32) + ReLU + Dropout(0.4)
   ‚Üì
Dense(32 ‚Üí 1)
   ‚Üì
Output: 1 prediction

Total: ~3,713 param√®tres
Normalisation: MinMaxScaler fit sur train
Early stopping: patience=30
Learning rate: 0.0005
```

---

## Sources du code analys√©

- `src/neural_network.py` - Architecture du mod√®le MLP
- `src/train_neural_network.py` - Script d'entra√Ænement
- `notebooks/neural_network_training.ipynb` - Notebook de d√©monstration
- Configuration par d√©faut : `NeuralNetworkConfig` (lignes 31-91 de `neural_network.py`)
