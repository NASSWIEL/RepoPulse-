{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20244445",
   "metadata": {},
   "source": [
    "# ðŸ§  Neural Network Training for GitHub Commit Prediction\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Training a neural network model for predicting GitHub commits\n",
    "2. Evaluating with RÂ² and MSE metrics\n",
    "3. Visualizing training losses and predictions\n",
    "4. Saving checkpoints for inference\n",
    "5. Building an interactive dashboard for real-time prediction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ad84ba",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8eaeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check for PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    TORCH_AVAILABLE = True\n",
    "    print(\"âœ… PyTorch available\")\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"âš ï¸ PyTorch not available - using sklearn fallback\")\n",
    "\n",
    "# Import project modules\n",
    "from src.etl import load_repository_data, list_available_repositories\n",
    "\n",
    "print(f\"âœ… Libraries imported successfully\")\n",
    "print(f\"ðŸ“Š NumPy: {np.__version__}\")\n",
    "print(f\"ðŸ“Š Pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29702c63",
   "metadata": {},
   "source": [
    "## 2. Define Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ace0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class NNConfig:\n",
    "    \"\"\"Neural Network Configuration\"\"\"\n",
    "    LOOKBACK_WINDOW = 12  # Number of past weeks to use as features\n",
    "    HIDDEN_LAYERS = [128, 64, 32]  # Hidden layer sizes\n",
    "    DROPOUT_RATE = 0.2\n",
    "    LEARNING_RATE = 0.001\n",
    "    EPOCHS = 200\n",
    "    BATCH_SIZE = 32\n",
    "    EARLY_STOPPING_PATIENCE = 15\n",
    "    TEST_SIZE = 0.2\n",
    "\n",
    "if TORCH_AVAILABLE:\n",
    "    class CommitPredictor(nn.Module):\n",
    "        \"\"\"PyTorch Neural Network for commit prediction\"\"\"\n",
    "        \n",
    "        def __init__(self, input_size, hidden_layers=[128, 64, 32], dropout_rate=0.2):\n",
    "            super(CommitPredictor, self).__init__()\n",
    "            \n",
    "            layers = []\n",
    "            prev_size = input_size\n",
    "            \n",
    "            for hidden_size in hidden_layers:\n",
    "                layers.append(nn.Linear(prev_size, hidden_size))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "                prev_size = hidden_size\n",
    "            \n",
    "            layers.append(nn.Linear(prev_size, 1))\n",
    "            self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.network(x)\n",
    "    \n",
    "    print(\"âœ… PyTorch model architecture defined\")\n",
    "else:\n",
    "    print(\"âš ï¸ Using sklearn MLPRegressor as fallback\")\n",
    "\n",
    "# Print configuration\n",
    "print(f\"\\nðŸ“‹ Configuration:\")\n",
    "print(f\"   Lookback Window: {NNConfig.LOOKBACK_WINDOW} weeks\")\n",
    "print(f\"   Hidden Layers: {NNConfig.HIDDEN_LAYERS}\")\n",
    "print(f\"   Dropout Rate: {NNConfig.DROPOUT_RATE}\")\n",
    "print(f\"   Learning Rate: {NNConfig.LEARNING_RATE}\")\n",
    "print(f\"   Max Epochs: {NNConfig.EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb19d62f",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b12cb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data from repositories\n",
    "import os\n",
    "\n",
    "def load_repository_data(repo_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load commits data from a repository.\"\"\"\n",
    "    commits_file = os.path.join(repo_path, \"commits.csv\")\n",
    "    if os.path.exists(commits_file):\n",
    "        df = pd.read_csv(commits_file)\n",
    "        if 'date' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "            df = df.dropna(subset=['date'])\n",
    "            return df\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def aggregate_weekly(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate commits by week.\"\"\"\n",
    "    if df.empty or 'date' not in df.columns:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['week'] = df['date'].dt.to_period('W').dt.start_time\n",
    "    weekly = df.groupby('week').size().reset_index(name='commits')\n",
    "    weekly = weekly.sort_values('week')\n",
    "    return weekly\n",
    "\n",
    "def create_sequences(data: np.ndarray, lookback: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Create sequences for time series prediction.\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - lookback):\n",
    "        X.append(data[i:i+lookback])\n",
    "        y.append(data[i+lookback])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load data from facebook/react repository\n",
    "repo_base = \"../repositories\"\n",
    "repo_name = \"facebook__react\"\n",
    "repo_path = os.path.join(repo_base, repo_name)\n",
    "\n",
    "print(f\"Loading data from {repo_name}...\")\n",
    "df = load_repository_data(repo_path)\n",
    "print(f\"Total commits: {len(df)}\")\n",
    "\n",
    "weekly_data = aggregate_weekly(df)\n",
    "print(f\"Weekly aggregated data points: {len(weekly_data)}\")\n",
    "weekly_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18c4a88",
   "metadata": {},
   "source": [
    "## 4. Train Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3111633",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkTrainer:\n",
    "    \"\"\"Neural network trainer with tracking and visualization.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: NNConfig):\n",
    "        self.config = config\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.model = None\n",
    "        self.history = {'train_loss': [], 'val_loss': [], 'train_r2': [], 'val_r2': []}\n",
    "        \n",
    "    def prepare_data(self, weekly_data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Prepare training and validation data.\"\"\"\n",
    "        commits = weekly_data['commits'].values.reshape(-1, 1)\n",
    "        scaled = self.scaler.fit_transform(commits).flatten()\n",
    "        \n",
    "        X, y = create_sequences(scaled, self.config.lookback)\n",
    "        \n",
    "        # Split into train/val\n",
    "        split_idx = int(len(X) * 0.8)\n",
    "        X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "        \n",
    "        return X_train, X_val, y_train, y_val\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, verbose=True):\n",
    "        \"\"\"Train the neural network model.\"\"\"\n",
    "        from sklearn.neural_network import MLPRegressor\n",
    "        from sklearn.metrics import mean_squared_error, r2_score\n",
    "        \n",
    "        # Create model\n",
    "        self.model = MLPRegressor(\n",
    "            hidden_layer_sizes=tuple(self.config.hidden_layers),\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            alpha=0.0001,\n",
    "            learning_rate_init=self.config.learning_rate,\n",
    "            max_iter=1,\n",
    "            warm_start=True,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(self.config.epochs):\n",
    "            # Partial fit\n",
    "            self.model.partial_fit(X_train, y_train)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            y_train_pred = self.model.predict(X_train)\n",
    "            y_val_pred = self.model.predict(X_val)\n",
    "            \n",
    "            train_loss = mean_squared_error(y_train, y_train_pred)\n",
    "            val_loss = mean_squared_error(y_val, y_val_pred)\n",
    "            train_r2 = r2_score(y_train, y_train_pred)\n",
    "            val_r2 = r2_score(y_val, y_val_pred)\n",
    "            \n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['train_r2'].append(train_r2)\n",
    "            self.history['val_r2'].append(val_r2)\n",
    "            \n",
    "            if verbose and epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{self.config.epochs} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, Train RÂ²: {train_r2:.4f}, Val RÂ²: {val_r2:.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= self.config.patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "        return self.history\n",
    "\n",
    "# Prepare data\n",
    "trainer = NeuralNetworkTrainer(config)\n",
    "X_train, X_val, y_train, y_val = trainer.prepare_data(weekly_data)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Feature dimension: {X_train.shape[1]}\")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nðŸš€ Starting training...\")\n",
    "history = trainer.train(X_train, y_train, X_val, y_val)\n",
    "print(\"\\nâœ… Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b0e837",
   "metadata": {},
   "source": [
    "## 5. Plot Training Losses and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf0212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(history: dict, title: str = \"Training Progress\"):\n",
    "    \"\"\"Plot training loss and RÂ² curves.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Plot Loss\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "    ax1.plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('MSE Loss', fontsize=12)\n",
    "    ax1.set_title('Training and Validation Loss', fontsize=14)\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Plot RÂ²\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(epochs, history['train_r2'], 'b-', label='Train RÂ²', linewidth=2)\n",
    "    ax2.plot(epochs, history['val_r2'], 'r-', label='Validation RÂ²', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('RÂ² Score', fontsize=12)\n",
    "    ax2.set_title('Training and Validation RÂ²', fontsize=14)\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Plot training curves\n",
    "fig = plot_training_curves(history, f\"Neural Network Training: {repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfb769b",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model with RÂ² and MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ff40a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Get final predictions\n",
    "y_train_pred = trainer.model.predict(X_train)\n",
    "y_val_pred = trainer.model.predict(X_val)\n",
    "\n",
    "# Inverse transform to original scale\n",
    "y_train_actual = trainer.scaler.inverse_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_train_pred_actual = trainer.scaler.inverse_transform(y_train_pred.reshape(-1, 1)).flatten()\n",
    "y_val_actual = trainer.scaler.inverse_transform(y_val.reshape(-1, 1)).flatten()\n",
    "y_val_pred_actual = trainer.scaler.inverse_transform(y_val_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = {\n",
    "    'Training': {\n",
    "        'RÂ² Score': r2_score(y_train_actual, y_train_pred_actual),\n",
    "        'MSE': mean_squared_error(y_train_actual, y_train_pred_actual),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_train_actual, y_train_pred_actual)),\n",
    "        'MAE': mean_absolute_error(y_train_actual, y_train_pred_actual)\n",
    "    },\n",
    "    'Validation': {\n",
    "        'RÂ² Score': r2_score(y_val_actual, y_val_pred_actual),\n",
    "        'MSE': mean_squared_error(y_val_actual, y_val_pred_actual),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_val_actual, y_val_pred_actual)),\n",
    "        'MAE': mean_absolute_error(y_val_actual, y_val_pred_actual)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display metrics table\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ“Š MODEL EVALUATION METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Metric':<15} {'Training':>15} {'Validation':>15}\")\n",
    "print(\"-\" * 45)\n",
    "for metric in ['RÂ² Score', 'MSE', 'RMSE', 'MAE']:\n",
    "    train_val = metrics['Training'][metric]\n",
    "    val_val = metrics['Validation'][metric]\n",
    "    print(f\"{metric:<15} {train_val:>15.4f} {val_val:>15.4f}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Store for later use\n",
    "final_r2 = metrics['Validation']['RÂ² Score']\n",
    "final_mse = metrics['Validation']['MSE']\n",
    "print(f\"\\nâœ… Final Validation RÂ²: {final_r2:.4f}\")\n",
    "print(f\"âœ… Final Validation MSE: {final_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8623d2b5",
   "metadata": {},
   "source": [
    "## 7. Plot Actual vs Predicted Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d646a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_timeline(weekly_data: pd.DataFrame, y_val_actual: np.ndarray, \n",
    "                              y_val_pred_actual: np.ndarray, lookback: int, title: str):\n",
    "    \"\"\"Plot actual vs predicted commits timeline.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # Get validation period dates\n",
    "    val_start_idx = len(weekly_data) - len(y_val_actual)\n",
    "    val_dates = weekly_data['week'].iloc[val_start_idx:].values\n",
    "    \n",
    "    # Plot actual and predicted\n",
    "    ax.plot(val_dates, y_val_actual, 'b-', label='Actual Commits', linewidth=2, alpha=0.8)\n",
    "    ax.plot(val_dates, y_val_pred_actual, 'r--', label='Predicted Commits', linewidth=2, alpha=0.8)\n",
    "    \n",
    "    # Fill between for error visualization\n",
    "    ax.fill_between(val_dates, y_val_actual, y_val_pred_actual, alpha=0.2, color='purple')\n",
    "    \n",
    "    ax.set_xlabel('Week', fontsize=12)\n",
    "    ax.set_ylabel('Number of Commits', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Plot predictions timeline\n",
    "fig_timeline = plot_predictions_timeline(\n",
    "    weekly_data, y_val_actual, y_val_pred_actual, \n",
    "    config.lookback, f\"Commits Prediction: {repo_name} (Validation Set)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f2f11a",
   "metadata": {},
   "source": [
    "## 8. Save Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf052b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "def save_checkpoint(trainer, repo_name: str, metrics: dict, checkpoint_dir: str = \"../checkpoints\"):\n",
    "    \"\"\"Save model checkpoint with metadata.\"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint = {\n",
    "        'model': trainer.model,\n",
    "        'scaler': trainer.scaler,\n",
    "        'config': trainer.config,\n",
    "        'history': trainer.history,\n",
    "        'metrics': metrics,\n",
    "        'repo_name': repo_name,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'lookback': trainer.config.lookback\n",
    "    }\n",
    "    \n",
    "    # Save with repo name\n",
    "    safe_name = repo_name.replace('/', '_').replace('__', '_')\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"nn_model_{safe_name}.pkl\")\n",
    "    \n",
    "    with open(checkpoint_path, 'wb') as f:\n",
    "        pickle.dump(checkpoint, f)\n",
    "    \n",
    "    # Also save as latest\n",
    "    latest_path = os.path.join(checkpoint_dir, \"nn_model_latest.pkl\")\n",
    "    with open(latest_path, 'wb') as f:\n",
    "        pickle.dump(checkpoint, f)\n",
    "    \n",
    "    return checkpoint_path\n",
    "\n",
    "# Save the trained model\n",
    "checkpoint_path = save_checkpoint(trainer, repo_name, metrics)\n",
    "print(f\"âœ… Model checkpoint saved to: {checkpoint_path}\")\n",
    "print(f\"âœ… Latest checkpoint saved to: ../checkpoints/nn_model_latest.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a134a55f",
   "metadata": {},
   "source": [
    "## 9. Load Checkpoint and Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82f0d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_path: str) -> dict:\n",
    "    \"\"\"Load model checkpoint from file.\"\"\"\n",
    "    with open(checkpoint_path, 'rb') as f:\n",
    "        checkpoint = pickle.load(f)\n",
    "    return checkpoint\n",
    "\n",
    "def predict_next_weeks(checkpoint: dict, recent_data: np.ndarray, weeks_ahead: int = 4) -> np.ndarray:\n",
    "    \"\"\"Predict commits for the next N weeks.\"\"\"\n",
    "    model = checkpoint['model']\n",
    "    scaler = checkpoint['scaler']\n",
    "    lookback = checkpoint['lookback']\n",
    "    \n",
    "    # Scale input data\n",
    "    scaled_input = scaler.transform(recent_data.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    predictions = []\n",
    "    current_sequence = scaled_input[-lookback:].copy()\n",
    "    \n",
    "    for _ in range(weeks_ahead):\n",
    "        # Predict next value\n",
    "        next_pred = model.predict(current_sequence.reshape(1, -1))[0]\n",
    "        predictions.append(next_pred)\n",
    "        \n",
    "        # Update sequence\n",
    "        current_sequence = np.roll(current_sequence, -1)\n",
    "        current_sequence[-1] = next_pred\n",
    "    \n",
    "    # Inverse transform predictions\n",
    "    predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()\n",
    "    return predictions\n",
    "\n",
    "# Load checkpoint and run inference\n",
    "checkpoint = load_checkpoint(\"../checkpoints/nn_model_latest.pkl\")\n",
    "print(f\"âœ… Loaded checkpoint for: {checkpoint['repo_name']}\")\n",
    "print(f\"   Trained on: {checkpoint['timestamp']}\")\n",
    "\n",
    "# Get recent commits data\n",
    "recent_commits = weekly_data['commits'].values[-config.lookback:]\n",
    "print(f\"   Recent {config.lookback} weeks of commits: {recent_commits}\")\n",
    "\n",
    "# Predict next 4 weeks\n",
    "predictions = predict_next_weeks(checkpoint, recent_commits, weeks_ahead=4)\n",
    "print(f\"\\nðŸ“ˆ Predictions for next 4 weeks:\")\n",
    "for i, pred in enumerate(predictions, 1):\n",
    "    print(f\"   Week {i}: {pred:.0f} commits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daebada0",
   "metadata": {},
   "source": [
    "## 10. Display Saved Training Plots\n",
    "\n",
    "The training script has already generated plots. Let's display them from the saved files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d31fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import glob\n",
    "\n",
    "# Find saved training plots\n",
    "plots_dir = \"../training_plots\"\n",
    "plot_files = sorted(glob.glob(os.path.join(plots_dir, \"*.png\")))\n",
    "\n",
    "print(f\"ðŸ“Š Found {len(plot_files)} training plots:\\n\")\n",
    "\n",
    "for plot_file in plot_files:\n",
    "    print(f\"ðŸ“ˆ {os.path.basename(plot_file)}\")\n",
    "    display(Image(filename=plot_file, width=800))\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46645e3",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. **Neural Network Architecture**: MLPRegressor with hidden layers [128, 64, 32]\n",
    "2. **Time Series Prediction**: Using lookback window of 12 weeks\n",
    "3. **Training with Early Stopping**: Patience of 20 epochs\n",
    "4. **Evaluation Metrics**: RÂ² Score and MSE\n",
    "5. **Checkpoint Saving**: Model saved for inference in dashboard\n",
    "\n",
    "### Next Steps\n",
    "- Run the inference dashboard: `streamlit run src/inference_dashboard.py`\n",
    "- The dashboard allows inputting any GitHub URL for real-time commit prediction"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
